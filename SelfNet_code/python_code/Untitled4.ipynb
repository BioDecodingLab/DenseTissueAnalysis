{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852d2690-584a-4b56-a826-89b1d0b58c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu May 22 20:51:03 2025\n",
    "\n",
    "@author: Jorge\n",
    "\"\"\"\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tifffile import imread\n",
    "from cc3d import connected_components\n",
    "from stardist.matching import matching\n",
    "from tqdm import tqdm\n",
    "from skimage.segmentation import clear_border\n",
    "def normalize(img):\n",
    "    return (img-img.min())/(img.max()-img.min())\n",
    "\n",
    "def leer_imagenes(path):\n",
    "    \"\"\"Carga imágenes TIFF desde un directorio y las procesa como componentes conectados\"\"\"\n",
    "    names = sorted(glob(os.path.join(path, \"*.[tT][iI][fF]*\")))\n",
    "    if not names:\n",
    "        raise ValueError(f\"No se encontraron imágenes TIFF en {path}\")\n",
    "    print(f\"Imágenes leídas desde {path}: {len(names)}\")\n",
    "    images = [clear_border(connected_components(imread(name))) for name in names]\n",
    "    names = [os.path.basename(name).split(\".\")[0] for name in names]\n",
    "    return images, names\n",
    "\n",
    "def evaluate_models(gt_images, preds_dict, output_dir=\".\", thresholds=np.linspace(0.5, 1.0, 11), fmt = ['o-', 's-', '^-', 'o-', 's-', '^-']):\n",
    "    \"\"\"\n",
    "    Evalúa múltiples modelos de segmentación contra un ground truth y genera gráficos y resultados.\n",
    "    \n",
    "    Args:\n",
    "        gt_images (list): Lista de imágenes ground truth\n",
    "        preds_dict (dict): Diccionario {nombre_modelo: lista_de_imágenes_predichas}\n",
    "        output_dir (str): Directorio para guardar resultados\n",
    "        thresholds (array): Umbrales de IoU a evaluar\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (DataFrame con resultados, DataFrame con promedios, DataFrame con std)\n",
    "    \"\"\"\n",
    "    # Crear directorio de salida si no existe\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Evaluación con múltiples thresholds (para curvas AP)\n",
    "    ap_results = {}\n",
    "    std_results = {}\n",
    "    \n",
    "    for model_name, pred_images in preds_dict.items():\n",
    "        print(f\"\\nEvaluando {model_name}...\")\n",
    "        matches = [\n",
    "            [matching(gt_images[j], pred_images[j], thresh=i, report_matches=True) \n",
    "            for i in tqdm(thresholds, desc=f\"Thresholds {model_name}\")]\n",
    "            for j in range(len(gt_images))\n",
    "        ]\n",
    "        ap_values = [[m.accuracy for m in match] for match in matches]\n",
    "        ap_results[model_name] = np.mean(ap_values, axis=0)\n",
    "        std_results[model_name] = np.std(ap_values, axis=0)\n",
    "    \n",
    "    # Graficar curvas AP\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for idx, model_name in enumerate(preds_dict.keys()):\n",
    "        plt.errorbar(\n",
    "            thresholds, \n",
    "            ap_results[model_name], \n",
    "            yerr=std_results[model_name], \n",
    "            fmt=fmt[idx], \n",
    "            label=model_name, \n",
    "            capsize=5\n",
    "        )\n",
    "    plt.legend()\n",
    "    plt.ylabel(\"AP\")\n",
    "    plt.xlabel(r\"IoU threshold ($\\tau$)\")\n",
    "    plt.title(\"Detection Scores\")\n",
    "    plt.savefig(os.path.join(output_dir, \"detection_scores.png\"), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Evaluación con threshold fijo (0.5) para métricas detalladas\n",
    "    thresh = 0.5\n",
    "    detailed_results = []\n",
    "    \n",
    "    for model_name, pred_images in preds_dict.items():\n",
    "        print(f\"\\nEvaluando detalladamente {model_name} con IoU={thresh}...\")\n",
    "        matches = [matching(gt_images[j], pred_images[j], thresh=thresh) \n",
    "                  for j in tqdm(range(len(gt_images)))]\n",
    "        \n",
    "        # Convertir a DataFrame\n",
    "        keys = ['criterion', 'thresh', 'fp', 'tp', 'fn', 'precision', 'recall', \n",
    "                'accuracy', 'f1', 'n_true', 'n_pred', 'mean_true_score', \n",
    "                'mean_matched_score', 'panoptic_quality']\n",
    "        df = pd.DataFrame([m._asdict() for m in matches], columns=keys)\n",
    "        df.insert(0, 'Image', names)\n",
    "        df.insert(0, 'Model', model_name)\n",
    "        detailed_results.append(df)\n",
    "    \n",
    "    # Combinar todos los resultados\n",
    "    df_results = pd.concat(detailed_results, ignore_index=True)\n",
    "    \n",
    "    # Formatear columnas\n",
    "    float_cols = ['precision', 'recall', 'accuracy', 'f1', 'mean_true_score', \n",
    "                 'mean_matched_score', 'panoptic_quality']\n",
    "    int_cols = ['fp', 'tp', 'fn', 'n_true', 'n_pred']\n",
    "    \n",
    "    df_results[float_cols] = df_results[float_cols].astype(float).round(4)\n",
    "    df_results[int_cols] = df_results[int_cols].astype(int)\n",
    "    \n",
    "    # Renombrar columnas para mejor presentación\n",
    "    new_names = {\n",
    "        \"fp\": \"FP\", \"tp\": \"TP\", \"fn\": \"FN\",\n",
    "        \"precision\": \"Precision\", \"recall\": \"Recall\",\n",
    "        \"accuracy\": \"Average Precision\", \"f1\": \"F1-Score\",\n",
    "        \"n_true\": \"N True\", \"n_pred\": \"N Pred\"\n",
    "    }\n",
    "    df_results = df_results.rename(columns=new_names)\n",
    "    \n",
    "    # Calcular porcentajes\n",
    "    df_results['FP (%)'] = (df_results['FP'] / df_results['N Pred']).round(4)\n",
    "    df_results['TP (%)'] = (df_results['TP'] / df_results['N Pred']).round(4)\n",
    "    df_results['FN (%)'] = (df_results['FN'] / df_results['N Pred']).round(4)\n",
    "    \n",
    "    # Calcular promedios y std por modelo\n",
    "    metrics = ['FP (%)', 'TP (%)', 'FN (%)', 'Precision', 'Recall', \n",
    "               'Average Precision', 'F1-Score']\n",
    "    df_mean = df_results.groupby('Model', sort=False)[metrics].mean()\n",
    "    df_std = df_results.groupby('Model', sort=False)[metrics].std()\n",
    "    \n",
    "    # Guardar resultados\n",
    "    df_results.to_csv(os.path.join(output_dir, \"detailed_results.csv\"), sep=\";\", index=False)\n",
    "    df_mean.to_csv(os.path.join(output_dir, \"mean_results.csv\"), sep=\";\")\n",
    "    df_std.to_csv(os.path.join(output_dir, \"std_results.csv\"), sep=\";\")\n",
    "    \n",
    "    # Generar gráficos comparativos\n",
    "    plot_metrics_comparison(df_mean, df_std, output_dir)\n",
    "    plot_violin_metrics(df_results, metrics, output_dir)  # Nueva función para gráfico de violines\n",
    "    \n",
    "    return df_results, df_mean, df_std\n",
    "\n",
    "def plot_metrics_comparison(df_mean, df_std, output_dir):\n",
    "    \"\"\"Genera gráfico de comparación de métricas entre modelos\"\"\"\n",
    "    metrics = df_mean.columns\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.8 / len(df_mean)  # Ajustar ancho según número de modelos\n",
    "    \n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    for i, model in enumerate(df_mean.index):\n",
    "        plt.bar(\n",
    "            x + i * width,\n",
    "            df_mean.loc[model],\n",
    "            width,\n",
    "            yerr=df_std.loc[model],\n",
    "            label=model,\n",
    "            capsize=5,\n",
    "            alpha=0.8\n",
    "        )\n",
    "    \n",
    "    plt.xticks(x + width*(len(df_mean)/2-0.5), metrics, rotation=45)\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.title(\"Model Metrics Comparison (Mean ± STD)\")\n",
    "    plt.legend(title=\"Model\")\n",
    "    plt.grid(axis=\"y\", alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"metrics_comparison.png\"), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def plot_violin_metrics(df_results, metrics, output_dir):\n",
    "    \"\"\"\n",
    "    Genera gráfico de violines para visualizar distribución de métricas\n",
    "    \n",
    "    Args:\n",
    "        df_results (DataFrame): DataFrame con todos los resultados\n",
    "        metrics (list): Lista de métricas a visualizar\n",
    "        output_dir (str): Directorio para guardar el gráfico\n",
    "    \"\"\"\n",
    "    # Reestructurar los datos en formato \"long\" para Seaborn\n",
    "    long_df = df_results.melt(\n",
    "        id_vars=['Model'], \n",
    "        value_vars=metrics, \n",
    "        var_name='Métrica', \n",
    "        value_name='Valor'\n",
    "    )\n",
    "    \n",
    "    # Crear el gráfico de violín\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    # Gráfico de violín\n",
    "    sns.violinplot(\n",
    "        data=long_df, \n",
    "        x='Métrica', \n",
    "        y='Valor', \n",
    "        hue='Model', \n",
    "        inner=None,\n",
    "        palette=\"muted\",\n",
    "        split=False\n",
    "    )\n",
    "    \n",
    "    # Puntos individuales\n",
    "    sns.stripplot(\n",
    "        data=long_df, \n",
    "        x='Métrica', \n",
    "        y='Valor', \n",
    "        hue='Model', \n",
    "        dodge=True, \n",
    "        jitter=True, \n",
    "        color='black', \n",
    "        size=4, \n",
    "        alpha=0.5, \n",
    "        linewidth=1, \n",
    "        legend=False\n",
    "    )\n",
    "    \n",
    "    # Ajustes estéticos\n",
    "    plt.title(\"Distribución de Métricas por Modelo\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel(\"Valor\")\n",
    "    plt.xlabel(\"Métricas\")\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Mover la leyenda fuera del gráfico\n",
    "    plt.legend(\n",
    "        title=\"Modelos\", \n",
    "        bbox_to_anchor=(1.05, 1), \n",
    "        loc='upper left', \n",
    "        borderaxespad=0.\n",
    "    )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"violin_metrics.png\"), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Ejemplo de uso:\n",
    "if __name__ == \"__main__\":\n",
    "    # Cambiar al directorio del script\n",
    "    os.chdir(os.path.dirname(os.path.abspath(__file__)))\n",
    "    \n",
    "    # Cargar imágenes\n",
    "    gt, names = leer_imagenes(\"./gt\")\n",
    "    \n",
    "    # Definir modelos a comparar (nombre: path)\n",
    "    models_to_compare = {\n",
    "        \"CellPose\": \"./prediction_cellpose\",\n",
    "      \n",
    "    }\n",
    "    \n",
    "    # Cargar predicciones\n",
    "    preds_dict = {name: leer_imagenes(path)[0] for name, path in models_to_compare.items()}\n",
    "    \n",
    "    # Evaluar modelos\n",
    "    results, mean_results, std_results = evaluate_models(\n",
    "        gt, \n",
    "        preds_dict, \n",
    "        output_dir=\"\\\\WS1\\WS1_Remote_Disk\\Current Segovia Lab\\Emilio Gutiérrez\\Emilio Gutiérrez SOS\\test Cellpose\\test set\\results plots\",\n",
    "        thresholds=np.linspace(0.5, 1.0, 11),\n",
    "        fmt = ['o-', 's-', '^-', 'o-', 's-', '^-']\n",
    "    )\n",
    "    \n",
    "    print(\"\\nResultados promedio:\")\n",
    "    print(mean_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cellpose)",
   "language": "python",
   "name": "cellpose"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
