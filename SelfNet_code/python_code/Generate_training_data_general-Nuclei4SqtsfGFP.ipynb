{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9b48be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hmorales/miniconda3/envs/selfnet/lib/python3.9/site-packages/pydantic/_migration.py:281: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    }
   ],
   "source": [
    "from Supporting_functions import *\n",
    "from WBNS import WBNS_image\n",
    "import RedLionfishDeconv as rl\n",
    "\n",
    "import scipy.ndimage as ndi\n",
    "from aicsimageio.readers import CziReader\n",
    "from aicsimageio.writers import OmeTiffWriter\n",
    "import re\n",
    "\n",
    "from skimage import measure  \n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tifffile\n",
    "from tqdm import tqdm\n",
    "\n",
    "def custom_sort_images(file_name):\n",
    "    allparts = file_name.split('(')\n",
    "    parts = allparts[1].split(')')\n",
    "    return (parts[0].zfill(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c61d3cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "## Generate initial tiffs form czi \n",
    "\n",
    "# Initial data extraction , if False, use pre-cropped images\n",
    "extractImagesFromCzi = True                                               \n",
    "czi_folder_path = '/run/user/1000/gvfs/smb-share:server=134.34.176.179,share=pmtest_fast/Daniel/SqtsfGFP_dataforHernan/forAnalysis/231218_sqtsfGFP_singletrack_18deg/'\n",
    "ChannelId2Export = 1\n",
    "Nimages2Skip = 5\n",
    "blurWnd = 2 # for masking\n",
    "thres_crop = 2.0\n",
    "dataexperiment = '231218_sqtsfGFP_18deg'\n",
    "tempScale = 0.309723275 #TO-DO remove # Make image isotropic from czi to remove tempScale\n",
    "\n",
    "## Tiff source folders\n",
    "\n",
    "# Source paths for the tiff images if extractImagesFromCzi== True, the folders will be created\n",
    "srcpath  = r'/run/user/1000/gvfs/smb-share:server=134.34.176.179,share=pmtest_fast/Daniel/SqtsfGFP_dataforHernan//croppedfortraining/'\n",
    "psf_path = r'/run/user/1000/gvfs/smb-share:server=134.34.176.179,share=pmtest_fast/Daniel/SqtsfGFP_dataforHernan/Averaged_transformed_PSF_561.tif'\n",
    "\n",
    "dirOut    = r'/run/user/1000/gvfs/smb-share:server=134.34.176.179,share=pmtest_fast/Daniel/SqtsfGFP_dataforHernan/croppedfortraining/Models/Nuclei/'\n",
    "\n",
    "dirSource = srcpath + 'Nuclei_allData/'\n",
    "dirTarget = srcpath + 'Nuclei_allData_deconvolved/' \n",
    "\n",
    "\n",
    "## Generate image planes for training data \n",
    "\n",
    "\n",
    "## Process Target image\n",
    "processTargetImages = False\n",
    "# BG subtraction\n",
    "resolution_px = 10 # FWHM of the PSF\n",
    "resolution_pz = 0\n",
    "noise_lvl = 2\n",
    "# deconvolution\n",
    "padding = 32\n",
    "Niter = 10\n",
    "# post processing\n",
    "sigmaLoG = 0.0\n",
    "sigmaLoGAddScale = 0.0\n",
    "sigma = 0.8\n",
    "# image normalization\n",
    "thres_scale_target = 2.0 # threhold for simple mask for normalization\n",
    "percentiles_target = (20, 99.995)\n",
    "min_v_target = 0\n",
    "max_v_target = 65535\n",
    "# save processed images\n",
    "dirProcTarget = srcpath + 'Nuclei_allData_deconvolved/' \n",
    "\n",
    "## Process Source image\n",
    "# image normalization\n",
    "thres_scale_source = 1.5  # threhold for simple mask for normalization\n",
    "percentiles_source = (20, 99.999)\n",
    "min_v_source = 0\n",
    "max_v_source = 65535\n",
    "\n",
    "\n",
    "\n",
    "## Parameters for the patch selection\n",
    "patch_size = 64\n",
    "stride = 64\n",
    "signal_intensity_threshold = 10000  #parameter for selecting image patches containing signals\n",
    "\n",
    "xy_interval=2\n",
    "xz_interval=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ae3178",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8022ec45",
   "metadata": {},
   "source": [
    "# Generate initial tiffs from czi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352f9ee8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if extractImagesFromCzi == True:\n",
    "\n",
    "    # Create folders\n",
    "\n",
    "    createFolder(dirSource)\n",
    "    if dirSource != dirTarget:\n",
    "        createFolder(dirTarget)\n",
    "\n",
    "    # Get all tif images in the folder\n",
    "    image_names = sorted([f for f in os.listdir(czi_folder_path) if f.endswith('.czi')])\n",
    "    image_names.sort(key=custom_sort_images)\n",
    "    image_names = image_names[0::Nimages2Skip]\n",
    "\n",
    "\n",
    "    for i, image_name in enumerate(image_names):\n",
    "\n",
    "        start_time = time.time()  # Record the start time \n",
    "\n",
    "        # get image path \n",
    "        czi_file_path = os.path.join(czi_folder_path, image_name)   \n",
    "        reader = CziReader(czi_file_path)\n",
    "\n",
    "\n",
    "        match = re.search(r'\\((\\d+)\\)', image_name)\n",
    "        timeId = match.group(1)\n",
    "        timeId= timeId.zfill(3)\n",
    "        print('spim_TL'+str(timeId))\n",
    "\n",
    "        for view in range(reader.dims.V): \n",
    "            for color in range(reader.dims.C): \n",
    "\n",
    "                start_time = time.time()  # Record the start time \n",
    "\n",
    "                # Process only the channel of interest\n",
    "                if color == ChannelId2Export:\n",
    "\n",
    "                    # Open image\n",
    "                    lazy_t0 = reader.get_image_dask_data(\"ZYX\", V=view, C=color)  # returns 3D ZYX numpy array\n",
    "                    img = lazy_t0.compute()  # returns in-memory 3D numpy array\n",
    "                    img = img.astype(np.uint16)\n",
    "                    scale = reader.physical_pixel_sizes.X / reader.physical_pixel_sizes.Z\n",
    "                    print(img.shape)\n",
    "\n",
    "                    # Crop for calculations\n",
    "                    bounds_min, bounds_max, mask = get_image_cropping_box(img, blurWnd, scale, thres_crop)          \n",
    "                    img = img[bounds_min[0]:bounds_max[0], bounds_min[1]:bounds_max[1], bounds_min[2]:bounds_max[2]]\n",
    "\n",
    "                    # Make image isotropic\n",
    "                    img_shape = img.shape\n",
    "                    img = reslice(img,'xy',reader.physical_pixel_sizes.X,reader.physical_pixel_sizes.Z)\n",
    "                    new_img_shape = img.shape   \n",
    "                    new_physical_pixel_sizeZ = img_shape[0] * reader.physical_pixel_sizes.Z / new_img_shape[0]\n",
    "                    print(f\"image dimension from : {img_shape} to {new_img_shape}\")\n",
    "                    print(f\"z-space from : {reader.physical_pixel_sizes.Z} to {new_physical_pixel_sizeZ}\") \n",
    "\n",
    "                    # Save image\n",
    "                    img = img.astype(np.uint16)\n",
    "                    outName = 'spim'+dataexperiment+'_TL'+str(timeId)+'_Channel'+str(color)+'_Angle'+str(view)+'.tif'\n",
    "                    img_out = os.path.join(dirSource, outName)            \n",
    "                    tifffile.imwrite(      \n",
    "                        img_out,\n",
    "                        img,\n",
    "                        imagej=True, \n",
    "                        bigtiff=True,\n",
    "                        resolution=(1.0/reader.physical_pixel_sizes.X, 1.0/reader.physical_pixel_sizes.Y), \n",
    "                        metadata={'spacing': new_physical_pixel_sizeZ, 'unit': 'um', 'axes': 'ZYX'})\n",
    "\n",
    "        \n",
    "\n",
    "        Elapsed_time = time.time() - start_time\n",
    "        print(f\"Elapsed Time: {Elapsed_time:.4f} seconds, image {image_name}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab8f1bd",
   "metadata": {},
   "source": [
    "# Generate image planes for training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69713898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder: '/run/user/1000/gvfs/smb-share:server=134.34.176.179,share=pmtest_fast/Daniel/SqtsfGFP_dataforHernan/croppedfortraining/Models/Nuclei/' already exists\n",
      "folder: '/run/user/1000/gvfs/smb-share:server=134.34.176.179,share=pmtest_fast/Daniel/SqtsfGFP_dataforHernan/croppedfortraining/Models/Nuclei/raw_data/' already exists\n",
      "folder: '/run/user/1000/gvfs/smb-share:server=134.34.176.179,share=pmtest_fast/Daniel/SqtsfGFP_dataforHernan/croppedfortraining/Models/Nuclei/raw_data/xz/' already exists\n"
     ]
    }
   ],
   "source": [
    "## Create folders to export data\n",
    "outDir = dirOut+\"raw_data/\"\n",
    "xy_data = outDir+\"xy/\"\n",
    "xy_lr_data = outDir+\"xy_lr/\"\n",
    "xz_data = outDir+\"xz/\"\n",
    "   \n",
    "createFolder(dirOut)\n",
    "createFolder(outDir)\n",
    "createFolder(xy_data)\n",
    "createFolder(xy_lr_data)\n",
    "createFolder(xz_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967610ca",
   "metadata": {},
   "source": [
    "## Generate image planes for target and intermediate images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c21cbd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Processing image : spim231218_sqtsfGFP_18deg_TL190_Channel1_Angle0.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [00:21, 21.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 21.7682 seconds, image spim231218_sqtsfGFP_18deg_TL190_Channel1_Angle0.tif, 527 images exported \n",
      "** Processing image : spim231218_sqtsfGFP_18deg_TL195_Channel1_Angle0.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [00:46, 23.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 24.7243 seconds, image spim231218_sqtsfGFP_18deg_TL195_Channel1_Angle0.tif, 1103 images exported \n",
      "** Processing image : spim231218_sqtsfGFP_18deg_TL200_Channel1_Angle0.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [01:18, 27.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 32.3148 seconds, image spim231218_sqtsfGFP_18deg_TL200_Channel1_Angle0.tif, 1679 images exported \n",
      "** Processing image : spim231218_sqtsfGFP_18deg_TL205_Channel1_Angle0.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "4it [01:43, 26.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 24.3251 seconds, image spim231218_sqtsfGFP_18deg_TL205_Channel1_Angle0.tif, 2255 images exported \n",
      "** Processing image : spim231218_sqtsfGFP_18deg_TL210_Channel1_Angle0.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "5it [02:07, 25.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 24.8192 seconds, image spim231218_sqtsfGFP_18deg_TL210_Channel1_Angle0.tif, 2831 images exported \n",
      "** Processing image : spim231218_sqtsfGFP_18deg_TL215_Channel1_Angle0.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "6it [02:31, 25.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 23.9216 seconds, image spim231218_sqtsfGFP_18deg_TL215_Channel1_Angle0.tif, 3407 images exported \n",
      "** Processing image : spim231218_sqtsfGFP_18deg_TL220_Channel1_Angle0.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "7it [02:55, 24.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 23.2999 seconds, image spim231218_sqtsfGFP_18deg_TL220_Channel1_Angle0.tif, 3983 images exported \n",
      "** Processing image : spim231218_sqtsfGFP_18deg_TL225_Channel1_Angle0.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "8it [03:19, 24.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 24.5888 seconds, image spim231218_sqtsfGFP_18deg_TL225_Channel1_Angle0.tif, 4562 images exported \n",
      "** Processing image : spim231218_sqtsfGFP_18deg_TL230_Channel1_Angle0.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "9it [03:44, 24.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 24.4329 seconds, image spim231218_sqtsfGFP_18deg_TL230_Channel1_Angle0.tif, 5141 images exported \n",
      "** Processing image : spim231218_sqtsfGFP_18deg_TL235_Channel1_Angle0.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [04:07, 24.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 22.9451 seconds, image spim231218_sqtsfGFP_18deg_TL235_Channel1_Angle0.tif, 5723 images exported \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ** Images in dirSource were manually cropped\n",
    "\n",
    "# Get all tif images in the target folder, process and export them\n",
    "image_names = sorted([f for f in os.listdir(dirTarget) if f.endswith('.tif')])\n",
    "\n",
    "if processTargetImages == True:\n",
    "    #Creat output folder\n",
    "    createFolder(dirProcTarget)\n",
    "    # Open PSF and Prepare PSF\n",
    "    psf = tifffile.imread(psf_path)\n",
    "    psf_f = psf.astype(np.float32)\n",
    "    psf_norm = psf_f/psf_f.sum()\n",
    "\n",
    "    \n",
    "count = 1\n",
    "for i, image_name in tqdm(enumerate(image_names)):\n",
    "\n",
    "    start_time = time.time()  # Record the start time \n",
    "    print(f\"** Processing image : {image_name}\")\n",
    "    \n",
    "    # Open image and get metadata\n",
    "    img_path = os.path.join(dirTarget, image_name)   \n",
    "    img = tifffile.imread(img_path)\n",
    "    img_shape = img.shape\n",
    "    [physical_pixel_sizeX,physical_pixel_sizeY,physical_pixel_sizeZ] = read_tiff_voxel_size(img_path)    \n",
    "    scale = physical_pixel_sizeX / physical_pixel_sizeZ\n",
    "    \n",
    "    \n",
    "    if processTargetImages == True:\n",
    "        # Get mask\n",
    "        mask = get_image_simple_mask(img, sigma, scale, thres_scale_target)  \n",
    "        mask =  mask.astype(np.int16)\n",
    "\n",
    "        # Make image isotropic\n",
    "        if abs(1.0-scale) > 1e-4: \n",
    "            img = reslice(img,'xy',physical_pixel_sizeX,physical_pixel_sizeZ)\n",
    "        img = img.astype(np.float32)\n",
    "        new_img_shape = img.shape   \n",
    "        new_physical_pixel_sizeZ = img_shape[0] * physical_pixel_sizeZ / new_img_shape[0]\n",
    "        print(f\"     -image dimension from : {img_shape} to {new_img_shape}\")\n",
    "        print(f\"     -z-space from : {physical_pixel_sizeZ} to {new_physical_pixel_sizeZ}\")\n",
    "        \n",
    "        # Deconvolution\n",
    "        if Niter > 0: \n",
    "            # Padding image\n",
    "            img = np.pad(img, padding, mode='reflect')\n",
    "            imgSizeGB = img.nbytes / (1024 ** 3)\n",
    "            print('     -size(GB) : ', imgSizeGB)\n",
    "            # GPU deconvolution\n",
    "            res_gpu = rl.doRLDeconvolutionFromNpArrays(img, psf, niter=Niter,resAsUint8=False)\n",
    "            # Removing padding\n",
    "            img = res_gpu[padding:-padding, padding:-padding, padding:-padding]\n",
    "\n",
    "        # Remove noise and BG\n",
    "        if resolution_px > 0:\n",
    "            img = WBNS_image(img, resolution_px, noise_lvl)\n",
    "            if resolution_pz > 0:\n",
    "                img_xz=np.transpose(img,[1,0,2])\n",
    "                img_xz = WBNS_image(img_xz, resolution_pz, 0)\n",
    "                img = np.transpose(img_xz,[1,0,2])\n",
    "        # LoG filter\n",
    "        if sigmaLoG > 0 :\n",
    "            imgBorders = ndi.gaussian_laplace(img, sigmaLoG)\n",
    "            imgBorders *= -1.0\n",
    "            #imgBorders[imgBorders < 0] = 0\n",
    "            imgBorders *= sigmaLoGAddScale\n",
    "            img += imgBorders\n",
    "        \n",
    "        # Smooth\n",
    "        if sigma > 0:\n",
    "            img = ndi.gaussian_filter(img, sigma)\n",
    "        \n",
    "        # Image Normalization\n",
    "        if percentiles_target[0] > 0 or percentiles_target[1] < 100:\n",
    "            low_thres, high_thres0 = getNormalizationThresholds(img, percentiles_target) # low thres in whole image\n",
    "            low_thres0, high_thres = getNormalizationThresholds(img * mask, percentiles_target) # high thres in FG\n",
    "            img = remove_outliers_image(img, low_thres, high_thres)\n",
    "\n",
    "        img = image_scaling(img, min_v_target, max_v_target, True)\n",
    "        img = img.astype(np.uint16)\n",
    "        \n",
    "        # Save processed image\n",
    "        img_out_name = os.path.join(dirProcTarget, image_name)            \n",
    "        tifffile.imwrite(      \n",
    "            img_out_name,\n",
    "            img,\n",
    "            imagej=True, \n",
    "            resolution=(1.0/physical_pixel_sizeX, 1.0/physical_pixel_sizeY), \n",
    "            metadata={'spacing': new_physical_pixel_sizeZ, 'unit': 'um', 'axes': 'ZYX'})\n",
    "        \n",
    "    # Generate intermediate images down sample and then upsample\n",
    "    img_lr = np.zeros_like(img)\n",
    "    z,y,x = img.shape    \n",
    "    new_y = round(y * tempScale)\n",
    "    new_x = round(x * tempScale)\n",
    "    \n",
    "    for i in range(z):\n",
    "        temp_img  = cv2.resize(img[i,:,:],(new_x,new_y),interpolation=cv2.INTER_CUBIC)\n",
    "        #img_lr[i,:,:] = cv2.resize(temp_img,(x,y),interpolation=cv2.INTER_CUBIC)  \n",
    "        img_lr[i,:,:] = cv2.resize(temp_img,(x,y),interpolation=cv2.INTER_LINEAR) \n",
    "            \n",
    "    # Export planes for target and intermediate images, each plane as a TIFF image\n",
    "        \n",
    "    for i in range(z):\n",
    "        # Get plane stats\n",
    "        outName_target = f\"{xy_data}{count}.tif\"\n",
    "        outName_interm = f\"{xy_lr_data}{count}.tif\"\n",
    "        tifffile.imwrite(outName_target, img[i,:,:])\n",
    "        tifffile.imwrite(outName_interm, img_lr[i,:,:])      \n",
    "        count += 1\n",
    "\n",
    "    Elapsed_time = time.time() - start_time\n",
    "    print(f\"Elapsed Time: {Elapsed_time:.4f} seconds, image {image_name}, {count-1} images exported \") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a048bbdd",
   "metadata": {},
   "source": [
    "## Generate image planes for source images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8efc585b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Processing image : spim231218_sqtsfGFP_18deg_TL190_Channel1_Angle0.tif\n",
      "     -threshold_value: 379.5\n",
      "Intensity Norm  from (0 , 65256) to  (0, 65535)  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [00:40, 40.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 40.4249 seconds, image spim231218_sqtsfGFP_18deg_TL190_Channel1_Angle0.tif, 824 images exported \n",
      "** Processing image : spim231218_sqtsfGFP_18deg_TL195_Channel1_Angle0.tif\n",
      "     -threshold_value: 376.5\n",
      "Intensity Norm  from (0 , 65252) to  (0, 65535)  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [01:23, 41.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 42.9121 seconds, image spim231218_sqtsfGFP_18deg_TL195_Channel1_Angle0.tif, 1679 images exported \n",
      "** Processing image : spim231218_sqtsfGFP_18deg_TL200_Channel1_Angle0.tif\n",
      "     -threshold_value: 384.0\n",
      "Intensity Norm  from (0 , 65250) to  (0, 65535)  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [02:06, 42.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 42.8609 seconds, image spim231218_sqtsfGFP_18deg_TL200_Channel1_Angle0.tif, 2534 images exported \n",
      "** Processing image : spim231218_sqtsfGFP_18deg_TL205_Channel1_Angle0.tif\n",
      "     -threshold_value: 388.5\n",
      "Intensity Norm  from (0 , 65247) to  (0, 65535)  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "4it [02:53, 44.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 47.6828 seconds, image spim231218_sqtsfGFP_18deg_TL205_Channel1_Angle0.tif, 3389 images exported \n",
      "** Processing image : spim231218_sqtsfGFP_18deg_TL210_Channel1_Angle0.tif\n",
      "     -threshold_value: 388.5\n",
      "Intensity Norm  from (0 , 65245) to  (0, 65535)  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "5it [03:37, 44.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 43.7239 seconds, image spim231218_sqtsfGFP_18deg_TL210_Channel1_Angle0.tif, 4244 images exported \n",
      "** Processing image : spim231218_sqtsfGFP_18deg_TL215_Channel1_Angle0.tif\n",
      "     -threshold_value: 388.5\n",
      "Intensity Norm  from (0 , 65244) to  (0, 65535)  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "6it [04:21, 44.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 43.9925 seconds, image spim231218_sqtsfGFP_18deg_TL215_Channel1_Angle0.tif, 5099 images exported \n",
      "** Processing image : spim231218_sqtsfGFP_18deg_TL220_Channel1_Angle0.tif\n",
      "     -threshold_value: 390.0\n",
      "Intensity Norm  from (0 , 65242) to  (0, 65535)  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "7it [05:08, 45.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 47.1929 seconds, image spim231218_sqtsfGFP_18deg_TL220_Channel1_Angle0.tif, 5954 images exported \n",
      "** Processing image : spim231218_sqtsfGFP_18deg_TL225_Channel1_Angle0.tif\n",
      "     -threshold_value: 387.0\n",
      "Intensity Norm  from (0 , 65240) to  (0, 65535)  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "8it [05:55, 45.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 47.0072 seconds, image spim231218_sqtsfGFP_18deg_TL225_Channel1_Angle0.tif, 6809 images exported \n",
      "** Processing image : spim231218_sqtsfGFP_18deg_TL230_Channel1_Angle0.tif\n",
      "     -threshold_value: 393.0\n",
      "Intensity Norm  from (0 , 65239) to  (0, 65535)  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "9it [06:40, 45.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 44.7900 seconds, image spim231218_sqtsfGFP_18deg_TL230_Channel1_Angle0.tif, 7664 images exported \n",
      "** Processing image : spim231218_sqtsfGFP_18deg_TL235_Channel1_Angle0.tif\n",
      "     -threshold_value: 391.5\n",
      "Intensity Norm  from (0 , 65245) to  (0, 65535)  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [07:30, 45.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 50.3011 seconds, image spim231218_sqtsfGFP_18deg_TL235_Channel1_Angle0.tif, 8519 images exported \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get all tif images in the dirSource folder, process and export them\n",
    "image_names = sorted([f for f in os.listdir(dirSource) if f.endswith('.tif')])\n",
    "\n",
    "count = 1\n",
    "for i, image_name in tqdm(enumerate(image_names)):\n",
    "\n",
    "    start_time = time.time()  # Record the start time \n",
    "    print(f\"** Processing image : {image_name}\")\n",
    "    \n",
    "    # Open image and get metadata\n",
    "    img_path = os.path.join(dirSource, image_name)   \n",
    "    img = tifffile.imread(img_path)\n",
    "    img_shape = img.shape\n",
    "    [physical_pixel_sizeX,physical_pixel_sizeY,physical_pixel_sizeZ] = read_tiff_voxel_size(img_path)    \n",
    "    scale = physical_pixel_sizeX / physical_pixel_sizeZ\n",
    " \n",
    "    # Get mask\n",
    "    mask = get_image_simple_mask(img, sigma, scale, thres_scale_source)  \n",
    "    mask =  mask.astype(np.int16)\n",
    "    \n",
    "    # Image Normalization\n",
    "    if percentiles_source[0] > 0 or percentiles_source[1] < 100:\n",
    "        low_thres, high_thres0 = getNormalizationThresholds(img, percentiles_source) # low thres in whole image\n",
    "        low_thres0, high_thres = getNormalizationThresholds(img * mask, percentiles_source) # high thres in FG\n",
    "        img = remove_outliers_image(img, low_thres, high_thres)\n",
    "  \n",
    "    img = image_scaling(img, min_v_source, max_v_source, True)\n",
    "    img = img.astype(np.uint16)\n",
    "\n",
    "    # reslice Image\n",
    "    img_xz = reslice(img,'xz',physical_pixel_sizeX,physical_pixel_sizeZ)\n",
    "    \n",
    "    z,y,x = img_xz.shape \n",
    "    \n",
    "    for i in range(z):\n",
    "        outName_target = f\"{xz_data}{count}.tif\"\n",
    "        tifffile.imwrite(outName_target, img_xz[i,:,:])\n",
    "        count += 1\n",
    "  \n",
    "    Elapsed_time = time.time() - start_time\n",
    "    print(f\"Elapsed Time: {Elapsed_time:.4f} seconds, image {image_name}, {count-1} images exported \")    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fc30eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01dc63a6",
   "metadata": {},
   "source": [
    "# Generate training data from image planes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c25ce7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder: '/run/user/1000/gvfs/smb-share:server=134.34.176.179,share=pmtest_fast/Daniel/SqtsfGFP_dataforHernan/croppedfortraining/Models/Nuclei/train_data/' already exists\n"
     ]
    }
   ],
   "source": [
    "# Create folder output \n",
    "train_data_path = os.path.join(dirOut, 'train_data/')\n",
    "createFolder(train_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c5c095ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2862/2862 [01:29<00:00, 31.94it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2840/2840 [00:36<00:00, 77.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize arrays\n",
    "\n",
    "xy = []\n",
    "xy_lr = []\n",
    "xz = []\n",
    "\n",
    "# Loop over lateral images\n",
    "file_list_xy = os.listdir(xy_data)\n",
    "for i in tqdm(range(0, len(file_list_xy), xy_interval)):\n",
    "    xy_img = tifffile.imread(xy_data + str(i + 1) + '.tif')\n",
    "    xy_lr_img = tifffile.imread(xy_lr_data + str(i + 1) + '.tif')\n",
    "    L0 = min(xy_img.shape[0], xy_lr_img.shape[0])\n",
    "    L1 = min(xy_img.shape[1], xy_lr_img.shape[1])\n",
    "    for m in range(0, L0 - patch_size + 1, stride):\n",
    "        for n in range(0, L1 - patch_size + 1, stride):\n",
    "            crop_xy    =    xy_img[m:m + patch_size, n:n + patch_size]\n",
    "            crop_xy_lr = xy_lr_img[m:m + patch_size, n:n + patch_size]\n",
    "            \n",
    "            if np.max(crop_xy) >= signal_intensity_threshold:\n",
    "                xy.append(crop_xy)\n",
    "                xy_lr.append(crop_xy_lr)\n",
    "\n",
    "# Loop over axial images   \n",
    "file_list_xz = os.listdir(xz_data)\n",
    "for i in tqdm(range(0, len(file_list_xz), xz_interval)):\n",
    "    xz_img = tifffile.imread(xz_data + str(i + 1) + '.tif')\n",
    "    for m in range(0, xz_img.shape[0] - patch_size + 1, stride):\n",
    "        for n in range(0, xz_img.shape[1] - patch_size + 1, stride):\n",
    "            crop_xz = xz_img[m:m + patch_size, n:n + patch_size]\n",
    "\n",
    "            if np.max(crop_xz) >= signal_intensity_threshold:\n",
    "                xz.append(crop_xz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e333ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96492\n",
      "96492\n",
      "87231\n"
     ]
    }
   ],
   "source": [
    "print(len(xy))\n",
    "print(len(xy_lr))\n",
    "print(len(xz))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ad39f7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96492, 64, 64) (96492, 64, 64) (87231, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "# Convert to arays and save\n",
    "\n",
    "xy = np.array(xy, dtype=np.float32)\n",
    "xy_lr = np.array(xy_lr, dtype=np.float32)\n",
    "xz = np.array(xz, dtype=np.float32)\n",
    "print(xy.shape, xy_lr.shape, xz.shape)\n",
    "\n",
    "np.savez(os.path.join(train_data_path, 'train_data.npz'), xy=xy, xy_lr=xy_lr, xz=xz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dcd22c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save tiff to double check\n",
    "\n",
    "import tifffile as tiff\n",
    "tiff.imwrite(os.path.join(train_data_path,'xy.tif'), xy)\n",
    "tiff.imwrite(os.path.join(train_data_path,'xy_lr.tif'), xy_lr)\n",
    "tiff.imwrite(os.path.join(train_data_path,'xz.tif'), xz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a501428f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
