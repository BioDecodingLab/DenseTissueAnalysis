import torch
import cv2
import numpy as np
import Self_net_architecture
import tifffile
import threading
import time
from tqdm import tqdm
import re

import os
import argparse
import torch.optim as optim
import pytorch_dataset
import checkpoint
import itertools
from ssim import SSIM
from Charbonnier_loss import L1_Charbonnier_loss

import scipy.ndimage as ndi
from scipy import stats
from skimage import io, filters, measure

from WBNS import WBNS_image

## Required functions : 

##Training


def adjust_lr(init_lr,optimizer,epoch,step,gamma):
    if (epoch+1)%step==0:
        times=(epoch+1)/step
        lr=init_lr*gamma**times
        for params in optimizer.param_groups:
            params['lr']=lr


def backward_D_basic(lambda_gan,netD, criterionGAN,real, fake,type,device):
    """Calculate GAN loss for the discriminator
           Parameters:
               netD (network)      -- the discriminator D
               real (tensor array) -- real images
               fake (tensor array) -- images generated by a generator
           Return the discriminator loss.
           We also call loss_D.backward() to calculate the gradients.
           """
    # Real
    pred_real = netD(real)
    loss_D_real = criterionGAN(pred_real, True)
    # Fake
    pred_fake = netD(fake.detach())
    loss_D_fake =criterionGAN(pred_fake, False)
    # Combined loss and calculate gradients
    if type=='lsgan' or type=='vanilla':
        loss_D = lambda_gan*(loss_D_real + loss_D_fake) * 0.5
        loss_D.backward()
        return loss_D

    else:
        print('warning in calculating loss_D')


def set_requires_grad(net, requires_grad=False):
    for param in net.parameters():
        param.requires_grad = requires_grad


def train(train_set,netD_A,netD_B,netG_A,netG_B,deblur_net,args,device,criterionGAN,criterionL1,SSIM_loss,optimD,optimG,optim_deblur):

    lambda_cycle = 1
    lambda_deblur=1
    lambda_feedback=0
    lambda_gan=0.1
    lambda_ssim=0.1


    netD_A.train()
    netD_B.train()

    netG_A.train()
    netG_B.train()
    deblur_net.train()

    fakeA_pool=pytorch_dataset.ImagePool(50)
    fakeB_pool = pytorch_dataset.ImagePool(50)

    Loss_D=0.0
    Loss_G_GAN=0.0
    Loss_G_cycle=0.0

    Loss_feedback=0.0
    Loss_deblur=0.0


    for epoch in range(args.epochs):
        param1 = optimD.param_groups[0]
        param2=optimG.param_groups[0]
        adjust_lr(args.learning_rate_D, optimD, epoch,15,0.5)
        adjust_lr(args.learning_rate_G, optimG, epoch,15,0.5)
        adjust_lr(args.learning_rate_G, optim_deblur, epoch,15,0.5)

        print('epoch:{},learning_rate_D:{}'.format(epoch+1,param1['lr']))
        print('epoch:{},learning_rate_G:{}'.format(epoch + 1, param2['lr']))


        if epoch+1>=2:
            lambda_feedback=0.1


        for i, data in enumerate(train_set):

            #degradation modeling

            A= data['xy_lr']
            B=data['xz']
            C=data['xy']

            A = A.to(device)
            B = B.to(device)
            C=C.to(device)

            fake_B=netG_A(A)      #syn_xz

            fake_A=netG_B(B)

            set_requires_grad(deblur_net,False)
            fake_C1 = deblur_net(fake_B)

            rec_A=netG_B(fake_B)

            rec_B=netG_A(fake_A)

            #optimize G

            set_requires_grad(netD_A, False)
            set_requires_grad(netD_B, False)

            optimG.zero_grad()

            # GAN loss D_A(G_A(A))
            loss_G_A = lambda_gan*criterionGAN(netD_A(fake_B), True)
            # GAN loss D_B(G_B(B))
            loss_G_B = lambda_gan*criterionGAN(netD_B(fake_A), True)


            # Forward cycle loss || G_B(G_A(A)) - A||


            loss_cycle_A = lambda_cycle*criterionL1(rec_A,A)+lambda_ssim*(1-SSIM_loss(rec_A,A))

            # Backward cycle loss || G_A(G_B(B)) - B||

            loss_cycle_B = lambda_cycle*criterionL1(rec_B,B)+lambda_ssim*(1-SSIM_loss(rec_B,B))


            loss_feedback = lambda_feedback*(criterionL1(fake_C1, C)+lambda_ssim*(1-SSIM_loss(fake_C1,C)))

            loss_G = loss_G_A + loss_G_B + loss_cycle_A + loss_cycle_B+loss_feedback

            loss_G.backward()
            optimG.step()

            #optimize D

            if (i+1)%2==0:
                set_requires_grad(netD_A, True)
                set_requires_grad(netD_B, True)

                optimD.zero_grad()

                fake_B = fakeB_pool.query(fake_B)

                loss_D_A =backward_D_basic(lambda_gan,netD_A, criterionGAN, B, fake_B, 'lsgan', device)

                fake_A = fakeA_pool.query(fake_A)
                loss_D_B = backward_D_basic(lambda_gan,netD_B,criterionGAN,A,fake_A,'lsgan',device)

                optimD.step()

            else:
                loss_D_A=torch.Tensor([0.0])
                loss_D_B=torch.Tensor([0.0])

            #Checkpoint

            Loss_D += loss_D_A.item() + loss_D_B.item()
            Loss_G_GAN+=loss_G_A.item()+loss_G_B.item()

            Loss_feedback+=loss_feedback.item()

            Loss_G_cycle+=loss_cycle_A.item()+loss_cycle_B.item()

            if (i + 1) % args.log_interval == 0:
                print('epochs: {} {}/{}  degradation modeling stage: loss_D:{:4f} , '
                      'loss_G: {:4f} loss_G_cycle:{:4f} loss_feedback:{:4f}'.
                format(epoch+1,

                (i + 1) * args.batch_size, len(train_set.dataset),
                Loss_D / (args.log_interval*lambda_gan),Loss_G_GAN/(args.log_interval*lambda_gan),
                Loss_G_cycle / (args.log_interval*lambda_cycle),Loss_feedback/(args.log_interval)))

                Loss_D = 0.0
                Loss_G_GAN = 0.0
                Loss_G_cycle = 0.0
                Loss_feedback=0.0

            #deblurring

            set_requires_grad(deblur_net, True)
            fake_B = netG_A(A)  # syn_lr_xy
            fake_C2 = deblur_net(fake_B.detach())

            optim_deblur.zero_grad()

            loss_deblur = lambda_deblur * criterionL1(fake_C2, C) + lambda_ssim * (1 - SSIM_loss(fake_C2, C))

            loss_deblur.backward()
            optim_deblur.step()

            # Checkpoint

            Loss_deblur += loss_deblur.item()

            if (i + 1) % args.log_interval == 0:
                print('epochs: {} {}/{}  deblurring stage :loss_deblur:{:4f}'.
                      format(epoch + 1,
                             (i + 1) * args.batch_size, len(train_set.dataset),Loss_deblur /(args.log_interval*lambda_deblur)))

                Loss_deblur = 0.0

            if (i + 1) % args.imshow_interval == 0:
                checkpoint.intermediate_output(deblur_net, device, args.path, args.min_v, args.max_v, epoch, (i+1)*args.batch_size)
                torch.save(deblur_net.state_dict(),args.path+'checkpoint/saved_models/' + 'deblur_net_{}_{}'.format(epoch + 1,(i+1)*args.batch_size) + '.pkl')
                torch.save(netD_A.state_dict(),args.path+'checkpoint/saved_models/' + 'netD_A_{}_{}'.format(epoch + 1,(i+1)*args.batch_size) + '.pkl')
                torch.save(netD_B.state_dict(),args.path+'checkpoint/saved_models/' + 'netD_B_{}_{}'.format(epoch + 1,(i+1)*args.batch_size) + '.pkl')
                torch.save(netG_A.state_dict(),args.path+'checkpoint/saved_models/' + 'netG_A_{}_{}'.format(epoch + 1,(i+1)*args.batch_size) + '.pkl')
                torch.save(netG_B.state_dict(),args.path+'checkpoint/saved_models/' + 'netG_B_{}_{}'.format(epoch + 1,(i+1)*args.batch_size) + '.pkl')

## Prediction

def reslice(img,position,x_res,z_res):
    scale=z_res/x_res
    z,y,x=img.shape
    new_z = round(z*scale)
    
    #Normalize image  to [0, 1] before extrapolating
    img_max = np.amax(img).astype(np.float32)
    img_normalized = img.astype(np.float32) / img_max

    if position=='xz':
        reslice_img=np.transpose(img_normalized,[1,0,2])
        scale_img=np.zeros((y,new_z,x),dtype=np.float32)
        for i in range(y):
            scale_img[i] = cv2.resize(reslice_img[i],(x,new_z),interpolation=cv2.INTER_CUBIC)

    elif position=='yz':
        reslice_img=np.transpose(img_normalized,[2,0,1])
        scale_img = np.zeros((x,new_z,y), dtype=np.float32)
        for i in range(x):
            scale_img[i] = cv2.resize(reslice_img[i],(y,new_z),interpolation=cv2.INTER_CUBIC)
            
    elif position=='xy':
        reslice_img=np.transpose(img_normalized,[1,0,2])
        scale_img=np.zeros((y,new_z,x),dtype=np.float32)
        for i in range(y):
            scale_img[i] = cv2.resize(reslice_img[i],(x,new_z),interpolation=cv2.INTER_CUBIC) 
        scale_img=np.transpose(scale_img,[1,0,2])    

    # Re scale intensities
    scale_img[scale_img < 0] = 0
    scale_img[scale_img > 1] = 1  
    rescaled_img = (scale_img * img_max).astype(np.uint16)
    
    
    return rescaled_img

def reslice_bysize(img,position,new_z):
    z,y,x=img.shape
    
    #Normalize image  to [0, 1] before extrapolating
    img_max = np.amax(img).astype(np.float32)
    img_normalized = img.astype(np.float32) / img_max

    if position=='xz':
        reslice_img=np.transpose(img_normalized,[1,0,2])
        scale_img=np.zeros((y,new_z,x),dtype=np.float32)
        for i in range(y):
            scale_img[i] = cv2.resize(reslice_img[i],(x,new_z),interpolation=cv2.INTER_CUBIC)

    elif position=='yz':
        reslice_img=np.transpose(img_normalized,[2,0,1])
        scale_img = np.zeros((x,new_z,y), dtype=np.float32)
        for i in range(x):
            scale_img[i] = cv2.resize(reslice_img[i],(y,new_z),interpolation=cv2.INTER_CUBIC)
            
    elif position=='xy':
        reslice_img=np.transpose(img_normalized,[1,0,2])
        scale_img=np.zeros((y,new_z,x),dtype=np.float32)
        for i in range(y):
            scale_img[i] = cv2.resize(reslice_img[i],(x,new_z),interpolation=cv2.INTER_CUBIC) 
        scale_img=np.transpose(scale_img,[1,0,2])    

    # Re scale intensities
    scale_img[scale_img < 0] = 0
    scale_img[scale_img > 1] = 1  
    rescaled_img = (scale_img * img_max).astype(np.uint16)
    
    
    return rescaled_img

def output_img(deblur_net,device,min_v,max_v,write_stack,raw_img,batch_size):
          
    deblur_net.eval()
    device_cpu = torch.device('cpu')
#    batch_size=32

    z_shape=raw_img.shape[0]
    idx = z_shape // batch_size
    res=z_shape-idx*batch_size

    input_img = (raw_img.astype(np.float32) - min_v) / (max_v - min_v)
    input_img[input_img > 1] = 1
    input_img[input_img < 0] = 0
    input_img = np.expand_dims(input_img, axis=1)

    input_tensor = torch.from_numpy(input_img)


    with tqdm(total=idx, desc="Predicting ", unit=" planes") as pbar:
        for ii in range(idx):
            
    #        print(str(ii), ' from ', str(idx))
            with torch.no_grad():
                test_tensor=input_tensor[ii * batch_size:(ii + 1) * batch_size].to(device)
                net_output = deblur_net(test_tensor)
    #            print('{}/{}'.format((ii + 1) * batch_size, z_shape))
            net_output = net_output.squeeze_(1).to(device_cpu).numpy()
            net_output = net_output * (max_v - min_v) + min_v
            net_output = np.clip(net_output, 0, max_v).astype(np.uint16)

            write_stack[ii * batch_size:(ii + 1) * batch_size] = net_output
            
            pbar.update(1)

    if res!=0:
        test_tensor = input_tensor[idx * batch_size:].to(device)
        with torch.no_grad():
            net_output = deblur_net(test_tensor)
#            print('{}/{}'.format(z_shape, z_shape))

        net_output = net_output.squeeze_(1).to(device_cpu).numpy()
        net_output = net_output * (max_v - min_v) + min_v
        net_output = np.clip(net_output, 0, max_v).astype(np.uint16)

        write_stack[idx * batch_size:] = net_output


    


class myThread (threading.Thread):
    def __init__(self, threadID, name,deblur_net,device,min_v,max_v,write_stack,raw_img, batch_size):
        threading.Thread.__init__(self)
        self.threadID = threadID
        self.name = name
        self.deblur_net=deblur_net
        self.device=device
        self.min_v=min_v
        self.max_v=max_v
        self.raw_img=raw_img
        self.write_stack=write_stack
        self.batch_size = batch_size

    def run(self):
#        print ("start threading：" + self.name)
        output_img(self.deblur_net, self.device, self.min_v,self.max_v,self.write_stack,self.raw_img, self.batch_size)
    
#        print ("quit threading：" + self.name)


def upsample_block(raw_img,x_res,z_res,deblur_netA,deblur_netB, min_v, max_v,device1, device2, batch_size, skip_planes=0):

    
    
    xz_img=reslice(raw_img,'xz',x_res,z_res)
    yz_img=reslice(raw_img,'yz',x_res,z_res)

    z1,y1,x1 = xz_img.shape
    z2,y2,x2 = yz_img.shape
    #print(xz_img.shape,yz_img.shape)
    
   
    if skip_planes > 1:
        xz_img = xz_img[::skip_planes, :, :]
        yz_img = yz_img[::skip_planes, :, :]
    
  
    out_xz_img=np.zeros_like(xz_img,dtype=np.uint16)
    out_yz_img=np.zeros_like(yz_img,dtype=np.uint16)

    
    thread1=myThread(1,'thread:1',deblur_netA,device1,min_v,max_v,out_xz_img,xz_img,batch_size)
    thread2=myThread(2,'thread:2',deblur_netB,device2,min_v,max_v,out_yz_img,yz_img,batch_size)

    thread1.start()
    thread1.join()
    
    thread2.start()
    thread2.join()

    
    if skip_planes > 1:
        out_xz_img=reslice_bysize(out_xz_img,'xy',z1)
        out_yz_img=reslice_bysize(out_yz_img,'xy',z2)

    
    re_out_xz=np.transpose(out_xz_img,[1,0,2])
    re_out_yz=np.transpose(out_yz_img,[1,2,0])
    

    fusion_stack=re_out_xz/2+re_out_yz/2
    fusion_stack=np.array(fusion_stack,dtype=np.uint16)


    return fusion_stack


## New functions


def predict_stack(img,deblur_net, min_v, max_v, device1, batch_size, type_out):


    out_img=np.zeros_like(img,dtype=np.uint16)

    thread1=myThread(1,'thread:1',deblur_net,device1,min_v,max_v,out_img,img,batch_size)

    thread1.start()
    thread1.join()

#    fusion_stack=np.array(out_img,dtype=np.uint16)

    fusion_stack=np.array(out_img,dtype=type_out)

    return fusion_stack



def upsample_block_3views(raw_img,x_res,z_res,deblur_netA,deblur_netB, min_v, max_v,device1, device2, device3, batch_size):

    xy_img=reslice(raw_img,'xy',x_res,z_res)
    xz_img=reslice(raw_img,'xz',x_res,z_res)
    yz_img=reslice(raw_img,'yz',x_res,z_res)

    print(xz_img.shape,yz_img.shape, xy_img.shape)

    out_xy_img=np.zeros_like(xy_img,dtype=np.uint16)
    out_xz_img=np.zeros_like(xz_img,dtype=np.uint16)
    out_yz_img=np.zeros_like(yz_img,dtype=np.uint16)

    thread1=myThread(1,'thread:1',deblur_netA,device1,min_v,max_v,out_xz_img,xz_img,batch_size)
    thread2=myThread(2,'thread:2',deblur_netB,device2,min_v,max_v,out_yz_img,yz_img,batch_size)
    thread3=myThread(3,'thread:3',deblur_netB,device3,min_v,max_v,out_xy_img,xy_img,batch_size)

    thread1.start()
    thread2.start()
    thread3.start()
    thread1.join()
    thread2.join()
    thread3.join()

    print(out_xz_img.shape,out_yz_img.shape, out_xy_img.shape)

    re_out_xz=np.transpose(out_xz_img,[1,0,2])
    re_out_yz=np.transpose(out_yz_img,[1,2,0])
    re_out_xy=out_xy_img

    print(re_out_xz.shape,re_out_yz.shape, re_out_xy.shape)

    fusion_stack=(re_out_xz/3)+(re_out_yz/3)+(re_out_xy/3)
    fusion_stack=np.array(fusion_stack,dtype=np.uint16)


    return fusion_stack




def getNormalizationThresholds(img, percentiles):

    # flalten array 
    if np.ndim(img) > 1:
        img = img.flatten()

    # get background thres
    low_thres = np.percentile(img,percentiles[0])
    # get foreground thres
    high_thres = np.percentile(img,percentiles[1])
    
    
    return low_thres, high_thres
    

def remove_outliers_image(img, low_thres, high_thres,print_res=False):

    if print_res == True:
        img_min = np.amin(img)
        img_max = np.amax(img)
    
    # Set the values in the image
    img[img > high_thres] = high_thres
    img = img - low_thres
    img[img < 0] = 0

    if print_res == True:
        newimg_min = np.amin(img)
        newimg_max = np.amax(img)
        print('Cropping Intensity from (%d , %d) to  (%d, %d) ' % (img_min, img_max, newimg_min, newimg_max))

    return img


def image_scaling(img, min_val, max_val, print_res=False):

    # start scaling
    img_shape = img.shape
    img_type = img.dtype
    img_min = np.amin(img)
    img_max = np.amax(img) 
              
    if img_shape[0] <  300:     
        img = np.reshape(img, newshape = -1)
        img = cv2.normalize(img, None, alpha = min_val, beta = max_val, norm_type = cv2.NORM_MINMAX, dtype = cv2.CV_32F)  
        img = np.reshape(img, newshape = img_shape)
    else:
        scale = img_max - img_min
        new_scale = max_val - min_val
        img = (new_scale * (img.astype(np.float32) - img_min) / scale) + min_val
    
    img = img.astype(img_type.name)

    if print_res == True:
        newimg_min = np.amin(img)
        newimg_max = np.amax(img)
        print('Intensity Norm  from (%d , %d) to  (%d, %d) ' % (img_min, img_max, newimg_min, newimg_max), '\n')
    
    return img


def image_normalizing(img, percentiles, min_val, max_val):
       
    if percentiles[0] > 0 or percentiles[1] < 100:
        low_thres, high_thres = getNormalizationThresholds(img, percentiles)
        img = remove_outliers_image(img, low_thres, high_thres)

    img = image_scaling(img, min_val, max_val)
    
    return img


def image_normalization(img, percentiles, thres_scale, min_v, max_v):
    if percentiles[0] > 0 or percentiles[1] < 100:
        # Get mask
        mask = get_image_simple_mask(img, 0.0, 1.0, thres_scale)  
        mask =  mask.astype(np.int16)
    
        low_thres, high_thres0 = getNormalizationThresholds(img, percentiles) # low thres in whole image
        low_thres0, high_thres = getNormalizationThresholds(img * mask, percentiles) # high thres in FG
        img = remove_outliers_image(img, low_thres, high_thres)

    img = image_scaling(img, min_v, max_v, True)
    img = img.astype(np.uint16)
    
    return img
    
    
def image_get_scaling(img, min_val, max_val):

    # start scaling
    img_shape = img.shape
    img_type = img.dtype
    img_min = np.amin(img)
    img_max = np.amax(img) 
              
    scale = img_max - img_min
    new_scale = max_val - min_val
    img = (new_scale * (img.astype(np.float32) - img_min) / scale) + min_val
    
    img = img.astype(img_type.name)

    
    return img, new_scale/scale


def get_image_cropping_box(img, blurWnd, scale, thres_scale):
    
    # get mask
    binary_image = get_image_simple_mask(img, blurWnd, scale, thres_scale)

    # Crop the subimage containing the largest object
    label_image = measure.label(binary_image, connectivity=1)
    regions = measure.regionprops(label_image)

    if len(regions) > 0:
        # Find the largest object by area
        largest_region = max(regions, key=lambda region: region.area)

        # Get the bounding box of the largest object
        min_1, min_2, min_3, max_1, max_2, max_3 = largest_region.bbox

        #print(f"min_1: {min_1}, max_1: {max_1}")
        #print(f"min_2: {min_2}, max_2: {max_2}")
        #print(f"min_3: {min_3}, max_3: {max_3}")
        bounds_min = [min_1, min_2, min_3]
        bounds_max = [max_1, max_2, max_3]
        
        
        return bounds_min, bounds_max, largest_region.image

    else:
        print("Error, no objects were found: reduce threshold") 

        
def get_image_simple_mask(img, blurWnd, scale, thres_scale):
  
    # Calculate the mode
    if blurWnd > 0:
        img_smooth = ndi.gaussian_filter(img, [blurWnd, blurWnd/scale,  blurWnd/scale])       
    else:
        img_smooth = img

    if img_smooth.shape != img.shape:
        img_smooth = img
        
    flattened_img = img_smooth.ravel()
    # Remove all zero values
    flattened_img = flattened_img[flattened_img != 0]
    mode_result = stats.mode(flattened_img)

    print("mode_result.mode : ", mode_result.mode)
    if(type(mode_result.mode) == np.uint16 or type(mode_result.mode) == np.float32):
         threshold_value = thres_scale*mode_result.mode
    else:
        threshold_value = thres_scale*mode_result.mode[0]
        
    print("     -threshold_value:", threshold_value)

    # get mask
    binary_mask = img_smooth > threshold_value 
    
    return binary_mask
    

def insert_predicted_image(img, fusion_stack, bounds_min, bounds_max,scale):
    
    scale_img = np.copy(img)
    minBoundz = int(bounds_min[0]/scale)
    maxBoundz =int(bounds_max[0]/scale)
    
    imgshape = fusion_stack.shape
    if (maxBoundz  - minBoundz) < imgshape[0]:
        maxBoundz = imgshape[0] + minBoundz
    
    if (bounds_max[1]  - bounds_min[1]) < imgshape[1]:
        bounds_max[1] = imgshape[1] + bounds_max[1]
        
    if (bounds_max[2]  - bounds_min[2]) < imgshape[2]:
        bounds_max[2] = imgshape[2] + bounds_max[2]
            
    scale_img[minBoundz:maxBoundz, bounds_min[1]:bounds_max[1], bounds_min[2]:bounds_max[2]] = fusion_stack
    
    return scale_img
                
def custom_save_img(fusion_stack, outdir, outName, pixel_sizesX, pixel_sizesY, pixel_sizesZ):

    fusion_stack = fusion_stack.astype(np.uint16)
    img_out = os.path.join(outdir, outName)            
    tifffile.imwrite(      
        img_out,
        fusion_stack,
        imagej=True, 
        #bigtiff=True,
        resolution=(1.0/pixel_sizesX, 1.0/pixel_sizesY), 
        metadata={'spacing': pixel_sizesZ, 'unit': 'um', 'axes': 'ZYX'})

## Accesory functions

def createFolder(outdir):
    if not os.path.exists(outdir):
        os.mkdir(outdir)
    else:
        print(f"folder: '{outdir}' already exists")
        

# Define a custom sorting function to sort the files
def custom_sort(file_name):
    allparts = file_name.split('.')
    parts = allparts[0].split('_')
    return (parts[0], parts[1], int(parts[2]),int(parts[3]))


# Get metada data tiff images

def read_tiff_voxel_size(file_path):
    """
    Implemented based on information found in https://pypi.org/project/tifffile
    """

    def _xy_voxel_size(tags, key):
        assert key in ['XResolution', 'YResolution']
        if key in tags:
            num_pixels, units = tags[key].value
            return units / num_pixels
        # return default
        return 1.

    with tifffile.TiffFile(file_path) as tiff:
        image_metadata = tiff.imagej_metadata
        if image_metadata is not None:
            z = image_metadata.get('spacing', 1.)
        else:
            # default voxel size
            z = 1.

        tags = tiff.pages[0].tags
        # parse X, Y resolution
        y = _xy_voxel_size(tags, 'YResolution')
        x = _xy_voxel_size(tags, 'XResolution')
        # return voxel size
        return [x, y, z]
